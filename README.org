* PARIETAL

PARIETAL: Yet another deeP leARnIng brain ExTrAtion tooL :dancers::dancers:

** Motivation: why another skull-stripping method?

During the last years, we have been using available state-of-the-art skull-stripping tools happily. However, in deep learning pipelines, most of the internal processes are computed very fast thanks to the use of GPUs (lesion segmentation, tissue segmentation, etc..), so brain extraction tends to be orders of magnitude slower than the rest of the GPU based pipeline processes. *The main motivation behind PARIETAL is to have a fast and robust skull-stripping method that could be incorporated in our deep learning pipelines.*

Fortunately, various brain MRI datasets have been released, such as the [[https://sites.google.com/view/calgary-campinas-dataset/home][Calgary-Campinas-359]] dataset, permitting researchers to train deep learning models that will hopefully improve both performance and processing time. Although different deep learning methods have been proposed already for accurate brain extraction. PARIETAL is yet another one, yielding fast and accurate outputs, regardless of the imaging brain extraction protocol. In order to validate our proposed method, we have carried different experiments using the trained model on the Campinas dataset, *analyzing the capability of the learned architecture on unseen data and varied image acquisition protocols*.


** Architecture:

PARIETAL is a patch-based residual network [[http://arxiv.org/abs/1606.06650][3D-UNET]] with ~10M parameters (see Figure). We have trained the model using the =silver-masks= provided by the [[https://sites.google.com/view/calgary-campinas-dataset/home][Calgary-Campinas-359]] dataset. This dataset consists of 359 images of healthy adults (29-80 years) acquired on Siemens, Philips and General Electric scanners at both 1.5T and 3T (see Souza et al. 2017 for more information about the dataset).
[[media/unet_architecture.png]]

*** Training/inference characteristics:
- Input modalities: =T1-w=
- Training patch size: =32x32x32=
- Training sampling: balanced training, same number of brain and skull samples (non-brain) after sampling at =16x16x16=
- Optimizer: =Adadelta=
- Training batch size: =32=
- Training epochs: =200=
- Train loss: =cross entropy=
- Early stopping: =50 epochs (based on validation DSC)=
- Inference patch size: =32x32x32=
- Inference sampling: =16x16x16=

** Installation:
We implemented PARIETAL in [[www.python.org][Python]] using the [[www.pytorch.org][Pytorch]] deep learning toolkit. All necessary packages can be installed from =pip= as follows:

#+begin_src python
pip install -r requirements
#+end_src

** How to use it:

*** As an standalone script:

To use PARIETAL as an standalone script, just run =./parietal --help= to see all the available options:

#+begin_src bash
/path/to/parietal --help
#+end_src


**** Mandatory parameters:
- input_scan (=--input_image=): T1-w nifti image to process
- output_scan (=--out_name=): Output name for the skull-stripped image

**** Optional parameters:
- binary threshold (=--threshold=): output threshold used to binarize the skull-stripped image (default=0.5)
- gpu use (=--gpu=): use GPU for faster inference (default=No)
- gpu number (=--gpu_number=): which GPU number to use (default=0)
- verbose (=--verbose=): show useful information

*** As a Python library:

To use PARIETAL as a [[www.python.org][Python]] library, just import the =BrainExtraction= class from your script.

#+begin_src python
from brain_extraction import BrainExtraction

b = BrainExtraction()

input_scan = 'tests/example/T1.nii.gz'
output_scan = 'tests/example/parietal_brainmask.nii.gz'

# The result is stored both in disk at output_scan path and returned
# as np.array
brainmask = b.run(input_scan, ouput_scan)
#+end_src

In order to facilitate its use in larger experiments, the method's options can be set by default from a configuration file stored at =config/config.cfg=. Class declaration arguments overwrite the default configuration:

#+begin_src python
[data]
normalize = True
out_threshold = 0.5
workers = 10

[model]
model_name = campinas_baseline_s2_multires
sampling_step = 16
patch_shape = 32
use_gpu = True
gpu_number = 0
#+end_src


*** GPU vs CPU use:
The model can run with both GPU or a decent CPU. In most of our experiments, PARIETAL can extract the brain out of T1-w image in less than 10 seconds when using GPU and about 2 minutes when running on the CPU (see performance experiments for a more complete analysis).

*** Docker version:
In order to reduce the hassle to install all the dependencies in your local machine, we also provide a [[www.docker.com][Docker]] version. Please follow the [[https://docs.docker.com/install/][guide]] to install [[www.docker.com][Docker]] for your operating system. If you are on Linux and you want to use the GPU capabilities of your local machine, please be sure that you install the [[https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)][nvidia-docker (version 2.0)]] packages.

Once [[www.docker.com][Docker]] is available in your system, install the minimum Python dependencies as:

#+begin_src python
pip install pyfiglet docker
#+end_src

Then, running PARIETAL is as easy as an standalone script: (note: the first time you run the script, this may take some time to run because it will download the Docker image locally in your system).

#+begin_src bash
/path/to/parietal_docker --help
#+end_src

**** Mandatory parameters:
- input_scan (=--input_image=): T1-w nifti image to process
- output_scan (=--out_name=): Output name for the skull-stripped image

**** Optional parameters:
- binary threshold (=--threshold=): output threshold used to binarize the skull-stripped image (default=0.5)
- gpu use (=--gpu=): use GPU for faster inference (default=No)
- gpu number (=--gpu_number=): which GPU number to use (default=0)
- verbose (=--verbose=): show useful information



** Performance:
We have compared the performance of PARIETAL with several publicly available state-of-the-art tools and also against some other deep learning methods. To do so, we have run PARIETAL on different public available datasets such as [[http://www.oasis-brains.org/][OASIS]], [[https://resource.loni.usc.edu/resources/atlases-downloads/][LPBA40]] and the [[https://sites.google.com/view/calgary-campinas-dataset/home][Campinas]] dataset.

*** Campinas dataset:

Performance evaluation against the 12 manual masks from the Campinas dasaset. We extract values for other methods from the [[https://doi.org/10.1016/j.artmed.2019.06.008][Lucena et al. 2019]] paper:

| method            |  Dice | Sensitivity | Specificity |
|-------------------+-------+-------------+-------------|
| [[https://github.com/ANTsX/ANTs][ANTs]]              | 95.93 |       94.51 |       99.70 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916306176?via%253Dihub][BEAST]]             | 95.77 |       93.84 |       99.76 |
| [[https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET/UserGuide][BET]]               | 95.22 |       98.26 |       99.13 |
| [[http://brainsuite.org/processing/surfaceextraction/bse/][BSE]]               | 90.48 |       91.44 |       98.64 |
| [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2408865/][HWA]]               | 91.66 |       99.93 |       97.83 |
| [[https://www.frontiersin.org/articles/10.3389/fninf.2013.00032/full][MBWSS]]             | 95.57 |       92.78 |       99.48 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916306176?via%253Dihub][OPTIBET]]           | 95.43 |       96.13 |       99.37 |
| [[https://sites.google.com/site/jeiglesias/ROBEX][ROBEX]]             | 95.61 |       98.42 |       99.13 |
| [[https://www.ncbi.nlm.nih.gov/pubmed/15250643][STAPLE (previous)]] | 96.80 |       98.98 |       99.38 |
|-------------------+-------+-------------+-------------|
| [[http://dx.doi.org/10.1016/j.neuroimage.2017.08.021][Silver-masks]]      | 97.13 |       96.82 |       99.70 |
|-------------------+-------+-------------+-------------|
| [[https://doi.org/10.1016/j.artmed.2019.06.008][CONSNet]]           | 97.18 |       98.91 |       99.46 |
| *PARIETAL*        | 97.23 |       96.73 |       97.75 |
|-------------------+-------+-------------+-------------|

*** LPBA40 dataset:

Performance evaluation against the 40 manual masks from the LPBA40 dasaset.  Values for the rest of the methods are extracted from the [[https://doi.org/10.1016/j.artmed.2019.06.008][Lucena et al. 2019]] paper:

| method                               |  Dice | Sensitivity | Specificity |
|--------------------------------------+-------+-------------+-------------|
| [[https://github.com/ANTsX/ANTs][ANTs]]                                 | 97.25 |       98.98 |       99.17 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916306176?via%253Dihub][BEAST]]                                | 96.30 |       94.06 |       99.76 |
| [[https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET/UserGuide][BET]]                                  | 96.62 |       97.23 |       99.27 |
| [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2408865/][HWA]]                                  | 92.51 |       99.89 |       97.02 |
| [[https://www.frontiersin.org/articles/10.3389/fninf.2013.00032/full][MBWSS]]                                | 96.24 |       94.40 |       99.68 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916306176?via%253Dihub][OPTIBET]]                              | 95.87 |       93.35 |       99.74 |
| [[https://sites.google.com/site/jeiglesias/ROBEX][ROBEX]]                                | 96.77 |       96.50 |       99.50 |
| [[https://www.ncbi.nlm.nih.gov/pubmed/15250643][STAPLE (previous)]]                    | 97.59 |       98.14 |       99.46 |
|--------------------------------------+-------+-------------+-------------|
| [[https://doi.org/10.1016/j.artmed.2019.06.008][CONSNet]] (Campinas model)             | 97.35 |       98.14 |       99.45 |
| [[https://doi.org/10.1016/j.artmed.2019.06.008][CONSNet]] (trained on LPBA40)          | 98.47 |       98.55 |       99.75 |
| [[https://ieeexplore.ieee.org/abstract/document/7961201][auto UNET Salehi]] (trained on LPBA40) | 97.73 |       98.31 |       99.48 |
| [[https://ieeexplore.ieee.org/abstract/document/7961201][Unet Salehi (trained on LPBA40)]]      | 96.79 |       97.22 |       99.34 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916000306?via%253Dihub][3DCNN Kleesiek]] (trained on LPBA40)   | 96.96 |       97.46 |       99.41 |
| *PARIETAL* (Campinas model)          | 97.25 |       96.10 |       98.40 |
|--------------------------------------+-------+-------------+-------------|

*** OASIS dataset:

Similar to the previous datasets, we also show the performance of PARIETAL against the 77 brainmasks of the OASIS dataset. Values for the rest of the methods are extracted from the [[https://doi.org/10.1016/j.artmed.2019.06.008][Lucena et al. 2019]] paper:


| method                              |  Dice | Sensitivity | Specificity |
|-------------------------------------+-------+-------------+-------------|
| [[https://github.com/ANTsX/ANTs][ANTs]]                                | 95.30 |       94.39 |       98.73 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916306176?via%253Dihub][BEAST]]                               | 92.46 |       86.76 |       99.70 |
| [[https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET/UserGuide][BET]]                                 | 93.50 |       92.63 |       98.10 |
| [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2408865/][HWA]]                                 | 93.95 |       98.36 |       96.12 |
| [[https://www.frontiersin.org/articles/10.3389/fninf.2013.00032/full][MBWSS]]                               | 90.24 |       84.09 |       99.35 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916306176?via%253Dihub][OPTIBET]]                             | 94.45 |       91.51 |        9.22 |
| [[https://sites.google.com/site/jeiglesias/ROBEX][ROBEX]]                               | 95.55 |       93.95 |       99.06 |
| [[https://www.ncbi.nlm.nih.gov/pubmed/15250643][STAPLE (previous)]]                   | 96.09 |       95.18 |       98.98 |
|-------------------------------------+-------+-------------+-------------|
| [[https://doi.org/10.1016/j.artmed.2019.06.008][CONSNet]] (Campinas model)            | 95.54 |       93.98 |       99.05 |
| [[https://doi.org/10.1016/j.artmed.2019.06.008][CONSNet]] (trained on OASIS)          | 97.14 |       97.45 |       98.88 |
| [[https://ieeexplore.ieee.org/abstract/document/7961201][auto UNET Salehi]] (trained on OASIS) | 97.62 |       98.66 |       98.77 |
| [[https://ieeexplore.ieee.org/abstract/document/7961201][Unet Salehi (trained on OASIS)]]      | 96.22 |       97.29 |       98.27 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916000306?via%253Dihub][3DCNN Kleesiek]] (trained on OASIS)   | 95.02 |       92.40 |       99.28 |
| *PARIETAL* (Campinas model)         | 92.55 |       87.40 |       98.51 |
|-------------------------------------+-------+-------------+-------------|

In contrast to the previous datasets, OASIS masks were not manually annotated, so the results of PARIETAL using the Campinas trained model were limited, mostly due to inconsistencies between labelling protocols :man_shrugging: (see Figure):

[[media/oasis_masks.png]]

To further illustrate such an issue, we retrained the model using the 77 brain masks of the OASIS dataset using a two-fold cross-validation methodology. We followed the same approach done in Kleesiek et al. 2016, Salehi et al. 2017 and Lucena et al. 2019, i.e. a two-fold cross-validation strategy for assessing our model. After retraining, the performance of PARIETAL was similar or better than other deep learning methods:

| method                              |  Dice | Sensitivity | Specificity |
|-------------------------------------+-------+-------------+-------------|
| [[https://doi.org/10.1016/j.artmed.2019.06.008][CONSNet]] (Campinas model)            | 95.54 |       93.98 |       99.05 |
| [[https://doi.org/10.1016/j.artmed.2019.06.008][CONSNet]] (trained on OASIS)          | 97.14 |       97.45 |       98.88 |
| [[https://ieeexplore.ieee.org/abstract/document/7961201][auto UNET Salehi]] (trained on OASIS) | 97.62 |       98.66 |       98.77 |
| [[https://ieeexplore.ieee.org/abstract/document/7961201][Unet Salehi (trained on OASIS)]]      | 96.22 |       97.29 |       98.27 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916000306?via%253Dihub][3DCNN Kleesiek]] (trained on OASIS)   | 95.02 |       92.40 |       99.28 |
| *PARIETAL* (Campinas model)         | 92.55 |       87.40 |       98.51 |
| *PARIETAL* (trained on OASIS)       | 97.99 |       97.84 |       98.14 |
|-------------------------------------+-------+-------------+-------------|


*** Processing time:

Finally, we analyze the processing time (in seconds) of the proposed architecture against other methods in the field. For the PARIETAL method, we show the processing times with/without loading the model in the GPU for each new sample. This is the case when the model is not used in =batch mode= (to implement).

Processing times from all methods, but PARIETAL, have been extracted from [[https://doi.org/10.1016/j.artmed.2019.06.008][Lucena et al. 2019]] paper, where the authors report the use of a workstation equipped with a =Xeon E3-1220 v3, 4x3.10Ghz, Intel)=. GPU resources are identical for all the deep learning methods (=NVIDIA TITAN-X GPU, 12GB)=.


| method                        | Campinas | OASIS | LPBA40 |
|-------------------------------+----------+-------+--------|
| [[https://github.com/ANTsX/ANTs][ANTs]]                          |     1378 |  1025 |   1135 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916306176?via%253Dihub][BEAST]]                         |     1128 |   944 |    905 |
| [[https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET/UserGuide][BET]]                           |        9 |     5 |      7 |
| [[http://brainsuite.org/processing/surfaceextraction/bse/][BSE]]                           |        2 |     1 |      1 |
| [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2408865/][HWA]]                           |      846 |   248 |    281 |
| [[https://www.frontiersin.org/articles/10.3389/fninf.2013.00032/full][MBWSS]]                         |      135 |    66 |     79 |
| [[https://www.sciencedirect.com/science/article/pii/S1053811916306176?via%253Dihub][OPTIBET]]                       |      773 |   579 |    679 |
| [[https://sites.google.com/site/jeiglesias/ROBEX][ROBEX]]                         |       60 |    53 |     57 |
|-------------------------------+----------+-------+--------|
| [[https://doi.org/10.1016/j.artmed.2019.06.008][CONSNet]] (GPU)                 |       25 |    18 |     36 |
| CONSNet (CPU)                 |      516 |   214 |    301 |
|-------------------------------+----------+-------+--------|
| *PARIETAL* (GPU)              |       12 |     7 |      9 |
| *PARIETAL* (GPU + model load) |       17 |    12 |     14 |
| *PARIETAL* (CPU)              |      129 |   122 |    141 |
|-------------------------------+----------+-------+--------|


** References:

1. Souza, R., Lucena, O., Garrafa, J., Gobbi, D., Saluzzi, M., Appenzeller, S., … Lotufo, R. (2017). An open, multi-vendor, multi-field-strength brain MR dataset and analysis of publicly available skull stripping methods agreement. NeuroImage, 170, 482–494. [[https://doi.org/10.1016/j.neuroimage.2017.08.021%20][(link)]]

2. Lucena, O., Souza, R., Rittner, L., Frayne, R., & Lotufo, R. (2019). Convolutional neural networks for skull-stripping in brain MR imaging using silver standard masks. Artificial Intelligence in Medicine, 98(August 2018), 48–58. [[ https://doi.org/10.1016/j.artmed.2019.06.008][(link)]]

3. Sadegh, S., Salehi, M., Member, S., Erdogmus, D., Member, S., Gholipour, A., & Member, S. (2017). Auto-context Convolutional Neural Network (Auto-Net) for Brain Extraction in Magnetic Resonance Imaging, 0062(c), 1–12. [[https://doi.org/10.1109/TMI.2017.2721362%20][(link)]]

4. Kleesiek, J., Urban, G., Hubert, A., Schwarz, D., Maier-Hein, K., Bendszus, M., & Biller, A. (2016). Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. NeuroImage, 129, 460–469. [[https://doi.org/10.1016/j.neuroimage.2016.01.024][(link)]]

** Versions:
- v0.1: first usable version
- v0.2: multi-resolution training
- V0.3: docker capabilities and paper cleanup
